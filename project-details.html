<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Project Details</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,
              500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">


</head>

<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

  <!-- ======= Header ======= -->
  <header id="header">
    <div class="d-flex flex-column">

      <div class="profile">
        <img src="assets/img/raj_profile.jpg" alt="" class="img-fluid rounded-circle">
        <h1 class="text-light"><a href="index.html">Raj Mudigonda</a></h1>
        <div class="social-links mt-3 text-center">
          <a href="http://www.linkedin.com/in/rajmudigonda893" class="linkedin"><i class="bx bxl-linkedin"></i></a>
          <a href="https://github.com/rajm893" class="github"><i class="bx bxl-github"></i></a>
          <a href="https://twitter.com/RajMudigonda1" class="twitter"><i class="bx bxl-twitter"></i></a>
        </div>
      </div>

      <nav id="navbar" class="nav-menu navbar">
        <ul>
          <li><a href="index.html#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
          <li><a href="index.html#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
          <li><a href="index.html#skills" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Skills</span></a></li>        
          <li><a href="index.html#edu_exp" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Education and Experience</span></a></li>
          <li><a href="index.html#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Projects</span></a></li>
          <li><a href="index.html#certification" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>Certifications</span></a></li>
          <li><a href="index.html#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
        </ul>
      </nav><!-- .nav-menu -->
    </div>
  </header><!-- End Header -->

  <main id="main">

    <!-- ======= Breadcrumbs ======= -->
    <section id="breadcrumbs" class="breadcrumbs">
      <div class="container">

        <div class="d-flex justify-content-between align-items-center">
          <h2>Project Details</h2>
          <ol>
            <li><a href="index.html">Home</a></li>
            <li>Project Details</li>
          </ol>
        </div>

      </div>
    </section><!-- End Breadcrumbs -->

    <!-- ======= Portfolio Details Section ======= -->
    <section id="project-1" class="portfolio-details">
      <div class="container">

        <div class="row gy-4">

          </div>
            <div class="portfolio-description">
              <h2><u>Predictive Analysis for Retail Item Shrinkage</u></h2>
              <div class="col-lg-8">
                <img src="assets/img/portfolio/item_shrinks_diag.png" alt="">
                <p>At Johnson Controls, a significant challenge surfaced with a key retail client: substantial item shrinkage leading to notable financial setbacks. The task was to forecast potential shrink events over a 4-week span, detailed into Morning, Afternoon, and Evening segments.</p>

                <h3>Project Highlights:</h3>

                <ul>
                    <li><strong>Objective:</strong> The goal was to devise a predictive mechanism for item shrink occurrences over an upcoming four-week interval, enabling the store to adopt preventive measures and optimize staffing and operations.</li>
                    <li><strong>Data Infrastructure:</strong> BigQuery served as the primary data warehousing solution, ensuring robust data management and scalability.</li>
                    <li><strong>AI &amp; Machine Learning:</strong> The Google AI Platform (now Vertex AI Platform) was employed for model training and deployment. An ensemble approach, combining the strengths of the Extra-trees regressor and kNN, was chosen for its superior predictive accuracy.</li>
                    <li><strong>Visualization &amp; Reporting:</strong> The AI platform's http-endpoint was seamlessly integrated with Looker, presenting a clear dashboard for the 4-week forecast.</li>
                    <li><strong>Impactful Outcome:</strong> The predictive strategy culminated in an 85% plunge in item losses. The foresight to discern potential shrink events a month ahead allowed for strategic resource allocation, substantially mitigating losses.</li>
                </ul>
               </div>

            </div>
          </div>

        </div>

      </div>
    </section><!-- End Portfolio Details Section -->

    <section id="project-2" class="portfolio-details">
      <div class="container">

        <div class="row gy-4">

          </div>
            <div class="portfolio-description">
              <h2><u>Retail Theft & Fraud Detection via Anomaly Analysis</u></h2>
              <div class="col-lg-8">
                <img src="assets/img/portfolio/anamolies_detect.png" alt="">
                

              <p>In the bustling environment of retail stores, maintaining security and preventing potential thefts is of paramount importance. 
                This project was sculpted to address the challenge posed by the frequent alarms generated by electronic article surveillance (EAS) systems. 
                Often, these alarms, rather than alerting against genuine thefts, were setting off false triggers, causing unwarranted interruptions in 
                daily operations.</p>

              <h3>Project Highlights:</h3>
              <ul>
                  <li><strong>Objective:</strong> The core of the project revolved around discerning genuine alarms from false ones, aiming to reduce disruptions and enhance store security.</li>
                  <li><strong>Data Warehouse & AI Platform:</strong> I employed Google's BigQuery as the primary data warehouse, and the subsequent model training and deployment took place on the Google AI Platform, now rebranded as Vertex AI Platform.</li>
                  <li><strong>Anomaly Detection Model:</strong> The chosen model for this project was XGBoost. This powerful algorithm was tailored to predict EAS alarms and, more importantly, establish an upper boundary for anomaly detection.</li>
                  <li><strong>Visualization & Insights:</strong> To ensure easy accessibility of insights, an endpoint from the AI platform model prediction was integrated with Looker. This dashboard elegantly displayed anomalies derived from the prediction results.</li>
                  <li><strong>Tangible Impact:</strong> One of the most rewarding outcomes was the significant 80% monthly reduction in item losses post-implementation of the insights from this project.</li>
              </ul>
              <p>With its impeccable integration of big data tools and machine learning, this project has not only streamlined retail operations but has also fortified store security, ensuring a harmonious shopping experience for customers.</p>

               </div>

            </div>
          </div>

        </div>

      </div>
    </section><!-- End Portfolio Details Section -->

    <section id="project-3" class="portfolio-details">
      <div class="container">

        <div class="row gy-4">


          </div>
            <div class="portfolio-description">
              <h2><u>Edge computing solution using Google IOT Core, Google PubSub, Dataflow and Bigquery</u></h2>
              <div class="col-lg-8">
                <img src="assets/img/portfolio/Edge_computing_Solution.png" alt="">
                <p>Within the modern retail landscape, edge devices deployed on-site play a pivotal role in video analytics, producing a wealth of output logs vital for data-driven decisions.</p>
                <h3>Project Overview:</h3>
                <ul>
                    <li><strong>Objective:</strong> The primary aim was to craft a seamless, automated pipeline to transfer logs from disparate retailers' edge devices directly into Google Cloud's BigQuery.</li>
                    <li><strong>Data Ingestion:</strong> To achieve near-real-time data capture, logs were ingested into Google IoT Core and PubSub using the MQTT bridge, ensuring prompt and accurate data availability.</li>
                    <li><strong>Processing & Storage:</strong> Google Dataflow stood out as the processing framework of choice, meticulously processing incoming logs and subsequently storing them within BigQuery.</li>
                    <li><strong>Outcome & Benefits:</strong> This streamlined approach led to a notable 25% reduction in latency. Furthermore, the automation significantly eased the overhead of data management and maintenance, bringing efficiency to the forefront.</li>
                </ul>
                <p>This endeavor underscores the importance of integrating edge computing with modern cloud infrastructure to harness data in the most efficient manner, optimizing both performance and resource allocation.</p>
               </div>

            </div>
          </div>

        </div>

      </div>
    </section><!-- End Portfolio Details Section -->


    <section id="project-4" class="portfolio-details">
      <div class="container">

        <div class="row gy-4">


          </div>
            <div class="portfolio-description">
              <h2><u>AI-Powered Document Data Extraction Solution</u></h2>
              <div class="col-lg-8">
                <img src="assets/img/portfolio/ocr_solution.png" alt="">
                <p>In the age of data-driven decision-making, leveraging information trapped within physical documents can prove tedious and time-consuming. Recognizing this challenge, an advanced, AI-powered solution was crafted for efficient document data extraction.</p>

                <h3>Project Highlights:</h3>
                
                <ul>
                    <li><strong>Objective:</strong> Craft a robust solution to extract critical textual data from a diverse range of digitized documents, both images and PDFs, ensuring flexibility for structured and unstructured formats.</li>
                    <li><strong>OCR Integration:</strong> The heart of the system beats with Tesseract 4, a leading OCR engine. However, understanding the nuances of various document qualities, prior image processing techniques—including blurring and noise reduction—were applied to ensure optimum text extraction.</li>
                    <li><strong>NLP Enhancement:</strong> Beyond mere extraction, the system delved deeper into the textual realm using Natural Language Processing. Incorporating NER (Named Entity Recognition) through Spacy and advanced Text Classification techniques, it efficiently segregated key data elements from generic document text.</li>
                    <li><strong>Multilingual Support:</strong> Recognizing global applications, the solution was further augmented to support multilingual text extraction, encompassing a broader clientele.</li>
                    <li><strong>Storage:</strong> Once processed, the extracted data fields found a structured home within MongoDB, facilitating easy retrieval and management.</li>
                    <li><strong>Impactful Results:</strong> The transformation was dramatic. What once took a laborious 3 minutes per document was slashed to a mere 4 seconds, marking a 45x efficiency gain. With the power to process over 900 documents hourly, manual data entry became a relic of the past, driving significant cost and time savings for clients.</li>
                </ul>
                
                <p>This initiative stands as a testament to the transformative power of AI in overhauling traditional business processes, driving efficiency, and streamlining operations.</p>
               </div>

            </div>
          </div>

        </div>

      </div>
    </section><!-- End Portfolio Details Section -->


    <section id="project-5" class="portfolio-details">
      <div class="container">

        <div class="row gy-4">


          </div>
            <div class="portfolio-description">
              <h2><u>Classification of Explosives Using Laser-Induced Breakdown Spectroscopy (LIBS)</u></h2>
              <div class="col-lg-8">
                <img src="assets/img/portfolio/explosives.png" alt="">
                <p>Explosives, laden with immense potential energy, release a vast array of energies upon detonation, manifesting as heat, light, sound, and pressure. Efficiently categorizing such explosives into their intrinsic types like HMX, RDX, NTO, PETN, and TNT is of paramount importance in ensuring safety and optimizing usage.</p>

                <h3>Project Overview:</h3>

                <ul>
                    <li><strong>Scientific Foundation:</strong> At the heart of this project is Laser-Induced Breakdown Spectroscopy (LIBS) - a powerful atomic emission spectroscopy technique. Utilizing high-energy laser pulses, the material under study is atomized and excited by focusing the laser onto its surface, leading to plasma formation.</li>
                    
                    <li><strong>Spectral Analysis:</strong> Each element from the periodic table uniquely represents itself through specific LIBS spectral peaks in the emitted light, captured using a spectrograph detector. By pinpointing these distinctive peaks, the chemical composition of a sample can be precisely determined. Specifically, for this study, the LIBS spectrum was analyzed in the wavelength range of 200-1000 nm, presenting a feature for each variation in amplitude or intensity.</li>
                    
                    <li><strong>Data Processing:</strong> With a massive feature set of 25,699 attributes extracted from LIBS spectrum analyses, there was a clear need for dimensionality reduction. Leveraging Principal Component Analysis (PCA), features were streamlined by focusing on those with the most significant variance.</li>
                    
                    <li><strong>Model Implementation:</strong> Subsequent to the data processing, both kNN and SVM algorithms were trained to achieve the core project aim: classifying the diverse set of explosives.</li>
                </ul>

                <p>By integrating advanced spectroscopy techniques with modern data processing and machine learning methods, this project encapsulates the essence of applied science in addressing real-world challenges.</p>

               </div>

            </div>
          </div>

        </div>

      </div>
    </section><!-- End Portfolio Details Section -->


    <section id="project-6" class="portfolio-details">
      <div class="container">

        <div class="row gy-4">


          </div>
            <div class="portfolio-description">
              <h2><u>Video Analytic Solution for Enhanced Retail Insights</u></h2>
              <div class="col-lg-8">
                <img src="assets/img/portfolio/ai_camera_usecase.png" alt="">
                <p>In the ever-evolving landscape of retail, insights derived from in-store behaviors have become paramount. With this vision, I spearheaded the development of an advanced video analytics solution. Harnessing the capabilities of IP cameras stationed strategically within retail spaces, this system delivers precise metrics on crucial parameters such as occupancy, age-gender demographics, adherence to social distancing norms, and instances of shelf sweep.</p>

                <h3>Key Aspects:</h3>
                <ul>
                    <li><strong>Integration with Cutting-Edge Technologies:</strong> The solution is architected around the GStreamer pipeline and seamlessly runs on the Intel OpenVINO platform, ensuring efficient video processing and analytics in real-time.</li>
                    
                    <li><strong>Design Excellence:</strong> By leveraging the Factory Design Pattern, a robust framework was crafted. At its core, the 'Use Case Manager' operates as a superclass. Each specific use case, whether it's occupancy monitoring or age-gender analysis, is implemented as an individual subclass. This design choice not only augments modularity but also streamlines future expansions and customizations.</li>
                    
                    <li><strong>Global Deployment & Impact:</strong> This bespoke solution has been adopted by numerous large-scale retail chains spread across Europe, Asia, and North America. By offering invaluable insights and actionable data, it has been instrumental in driving substantial annual revenues, to the tune of $55 million.</li>
                </ul>
            
                <p>Through the synergy of technology and strategic design, this video analytic solution offers a robust tool for retail chains to optimize store operations, enhance customer experiences, and maximize profitability.</p>
            
               </div>

            </div>
          </div>

        </div>

      </div>
    </section><!-- End Portfolio Details Section -->


    <section id="project-7" class="portfolio-details">
      <div class="container">

        <div class="row gy-4">


          </div>
            <div class="portfolio-description">
              <h2><u>MLOps with DVC and MLFLow</u></h2>
              <div class="col-lg-8">
                <img src="assets/img/portfolio/ML_pipeline.png" alt="">
                <p>In the rapidly advancing domain of machine learning, continuous integration and deployment (CI/CD) of machine learning pipelines remain paramount to ensure seamless model lifecycle management. To exemplify this intricate process, I architected a solution harnessing the capabilities of MLflow and DVC (Data Version Control).</p>
    
                <h3>Key Highlights:</h3>
                <ul>
                    <li><strong>Data Management:</strong> The backbone of any ML pipeline – the data – is meticulously tracked and managed using DVC. By integrating with Git, DVC efficiently tracks '.dvc' files, whereas the actual data sources are securely stored on Google Cloud Storage.</li>
                    <li><strong>Structured Model Pipelines:</strong> Every phase of the model pipeline, be it loading, splitting, training, or evaluation, is orchestrated in a step-by-step manner using the <code>dvc.yaml</code> file. To ensure flexibility and easy replication, configurations for the pipeline are sourced from the <code>params.yaml</code> config file.</li>
                    <li><strong>Versatile Deployment:</strong> The application's utility is extended in two distinct formats:
                        <ol>
                            <li><strong>API-Based Predictions:</strong> A streamlined service that delivers predictions directly via an API endpoint.</li>
                            <li><strong>Web UI-Based Predictions:</strong> A more visual approach, allowing users to obtain predictions through an interactive web interface.</li>
                        </ol>
                    </li>
                    <li><strong>Robust Backend & Deployment:</strong> The prediction server, designed using Flask, offers impeccable performance and is hosted on the Heroku cloud for global accessibility.</li>
                    <li><strong>Automated CI/CD using GitHub Actions:</strong> To ensure the integrity and reliability of the pipeline, Github Actions is integrated using the <code>ci-cd.yaml</code> workflow. The system ensures deployments occur post rigorous test checks, ensuring only the most stable versions are released.</li>
                </ul>
            
                <p><strong>Repository Access:</strong> The comprehensive project code and further insights can be accessed via <a href="https://github.com/rajm893/ML_model_deployment_using_DVC_and_MLFlow">GitHub Link</a>.</p>
            
               </div>

            </div>
          </div>
          
        </div>

      </div>
    </section><!-- End Portfolio Details Section -->


    <section id="project-8" class="portfolio-details">
      <div class="container">

        <div class="row gy-4">


          </div>
            <div class="portfolio-description">
              <h2><u>NLP based model building on Quora question pairs dataset with FastAPI deployment</u></h2>
              <div class="col-lg-8">
                <img src="assets/img/portfolio/Quora_diag.png" alt="">
                <p>In the domain of Machine Learning, the deployment of models is just as critical as their development. This project showcases a versatile deployment strategy across several Google Cloud Platform (GCP) services, ensuring scalability, responsiveness, and resilience.</p>

                <h3>Objective:</h3>
                <p>Highlight the diverse deployment methodologies for Machine Learning models across pivotal GCP services:</p>
                <ol>
                    <li>Google Compute Engine (A robust virtual machine environment).</li>
                    <li>Google App Engine (A serverless, scalable platform).</li>
                    <li>Google Kubernetes Engine (Optimized for deploying applications at scale).</li>
                </ol>

                <h3>Core Application: Question Pair Similarity</h3>
                <p>The application discerns whether a pair of provided questions essentially convey the same meaning. For instance:</p>
                <ul>
                    <li><strong>Question 1:</strong> 'How can I be a good geologist?'</li>
                    <li><strong>Question 2:</strong> 'What should I do to be a great geologist?'</li>
                    <li><strong>Expected Result:</strong> True</li>
                </ul>

                <h3>Technical Stack:</h3>
                <p>Utilizing the synergy of FastAPI with Docker, the application is encapsulated and uniformly deployed across the aforementioned GCP services, ensuring consistent performance and scalability.</p>

                <h3>Access &amp; Resources:</h3>
                <p>The full codebase, along with intricate details and insights, is available on <a href="#">GitHub Link</a>.</p>

               </div>

            </div>
          </div>

        </div>

      </div>
    </section><!-- End Portfolio Details Section -->


    <section id="project-9" class="portfolio-details">
      <div class="container">

        <div class="row gy-4">


          </div>
            <div class="portfolio-description">
              <h2><u>Ethereum Data Engineering Pipeline</u></h2>
              <div class="col-lg-8">
                <img src="assets/img/portfolio/etherium-data-pipeline.png" alt="">
                <p>This project represents a sophisticated data engineering pipeline designed to derive actionable insights from Ethereum's public dataset. By leveraging Google's BigQuery, I integrated the data with a DataProc cluster to perform deep dives using PySpark operations. The primary objective was to visualize Ethereum transaction trends over a year and understand specific patterns, especially related to smart contracts.</p>

                <h3>Key features of the project include:</h3>

                <ul>
                    <li><strong>Dataset Overview:</strong> The project analyzed a year-long dataset from September 2022 to September 2023, including:
                        <ul>
                            <li>Blocks: 2.61 million records</li>
                            <li>Contracts: 12.06 million records</li>
                            <li>Transactions: 383.94 million records</li>
                        </ul>
                    </li>
                    <li><strong>Analysis Highlights:</strong> A series of analyses were conducted ranging from monthly transaction trends to identifying the top miners based on block sizes. The results were visualized through bar plots, which showcased monthly transaction counts and average transaction values.</li>
                    <li><strong>Technology Stack:</strong> This pipeline uniquely combines the capabilities of Google BigQuery with a Dataproc cluster. The PySpark operations, pivotal to the project, were conducted within Vertex AI's managed notebook environment, optimized for PySpark on DataProc.</li>
                    <li><strong>End Result:</strong> The deep dive into Ethereum's dataset revealed critical insights into transaction patterns, understanding gas usage trends, and the intricacies of smart contracts over the period. All these insights are thoroughly documented in a Jupyter notebook, offering a comprehensive guide.</li>
                </ul>

                <p>This Ethereum Data Engineering Pipeline stands as a testament to the capabilities of modern big data tools and cloud infrastructure in dissecting vast amounts of data, extracting meaningful information, and presenting it in a visually compelling manner.</p>

               </div>

            </div>
          </div>

        </div>

      </div>
    </section><!-- End Portfolio Details Section -->



    
  </main><!-- End #main -->

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>
  <!-- Vendor JS Files -->
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/typed.js/typed.umd.js"></script>
  <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>